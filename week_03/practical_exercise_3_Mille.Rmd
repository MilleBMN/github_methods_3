---
title: "practical_exercise_3, Methods 3, 2021, autumn semester"
<<<<<<< HEAD
author: 'Mille Bryske'
date: "4/10-2021"
=======
author: '[FILL IN YOUR NAME]'
date: "[FILL IN THE DATE]"
>>>>>>> c78e6ff3b1a0c11ea82ef719e7fe86fbc152da69
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse)
```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame
```{r}
#Listing all the csv files
files <- list.files(path = "experiment_2",         
                    pattern = ".csv",
                    full.names = T)
#Creating empty data frame
df <- data.frame()

#Making for loop to make data frame
for (i in 1:length(files)){
  new_data <- read.csv(files[i], header = TRUE )
  df <- rbind(new_data, df)
}
```

2) Describe the data and construct extra variables from the existing variables  
    i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.
```{r}
df$correct <- ifelse(df$target.type == "even" & df$obj.resp == "e" | df$target.type == "odd" & df$obj.resp == "o", 1, 0)

```
    
    ii. describe what the following variables in the data frame contain, _trial.type_, _pas_, _trial_, _target.contrast_, _cue_, _task_, _target_type_, _rt.subj_, _rt.obj_, _obj.resp_, _subject_ and _correct_. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  
trial.type: Indicates whether subject is doing the staircase task (first experiment) or the follow-up experiment.
Should be class character, as it is a category.
pas: Indicates subjects response to trial on the Perceptual Awareness Scale (PAS). Takes a value between 1-4, and will therefore be treated as numeric.
trial: A numbered list for every trial the subject completes, i.e. presses e or o in either of the trial types., per subject. I should think character class for now (might change).
target.contrast: The contrast between the background and the digit (target). Between 0-1, treated as numeric.
cue: The specific cue pattern, will treat as character.
task: Whether cue pattern is 2 (singles), 4 (pairs) or 8 (quadruplets) digits. Will treat as character.
target.type: Whether target type is an odd or even number - will treat as character.
rt.subj: Reaction time for response to PAS pr. trail - will treat as numeric.
rt.obj: Reaction time for responding if target is even or odd - will treat as numeric.
obj.resp: Subjects response to target is either even or odd - will treat as character.
subject: Participant ID, ordered from 001. Treated as character.
correct: Whether subject answered correctly in the trail, 1 for correct and 0 for incorrect. Is logical (binary)

```{r}
#Changing to correct classes
ls.str(df)
df$pas <- as.numeric(df$pas)
df$trial <- as.character(df$trial)
df$target.contrast <- as.numeric(df$target.contrast)
df$cue <- as.character(df$cue)
df$rt.subj <- as.numeric(df$rt.subj)
df$rt.obj <- as.numeric(df$rt.obj)
df$target.contrast <- as.numeric(df$target.contrast)
df$correct <- as.integer(df$correct)
df$correct <- as.logical(df$correct)
```



    iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. Comment on the fits - do we have enough data to plot the logistic functions?  
```{r}
#Complete pooling model
df_staircase <- df %>% 
  filter(trial.type == "staircase")
logistic_model_pooling <- glm(correct ~ target.contrast, data = df_staircase, family = "binomial")
df_staircase$fitted_pooling <- fitted(logistic_model_pooling)

df_staircase %>% 
  ggplot(aes(x = target.contrast, y = fitted_pooling))+
  geom_point()+
  geom_line(aes(target.contrast, fitted_pooling, color = "red"))+ 
  facet_wrap(~subject) #all plots will have the same function, since I've pooled them all in the model 
```


```{r}
#No pooling model 
logistic_model_nopooling <- glm(correct ~ target.contrast*subject, data = df_staircase, family = "binomial")
df_staircase$fitted_nopooling <- fitted(logistic_model_nopooling)
df_staircase$fitted_nopooling2 <- logistic_model_nopooling$fitted.values

ggplot(df_staircase, aes(x = target.contrast, y = fitted_nopooling))+
  geom_point()+
  geom_line(aes(target.contrast, fitted_nopooling, color = "red"))+
  facet_wrap(~subject)+
  ylim(c(0,1))

```



    iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_ 
```{r}
pacman::p_load(lme4)
logistic_model_partialpooling <- glmer(correct ~ target.contrast + (target.contrast|subject), data = df_staircase, family = binomial)
df_staircase$fitted_partialpooling <- fitted(logistic_model_partialpooling)

df_staircase %>% 
  ggplot(aes(x = fitted_partialpooling, y = correct))+
  geom_point()+
  facet_wrap(~subject)

df_staircase %>% 
  ggplot()+ 
  geom_point(aes(x = target.contrast, y = fitted(logistic_model_pooling), color = "complete pooling")) + #should have used the no pooling model here
  geom_point(aes(x = target.contrast, y = fitted(logistic_model_partialpooling), color = "partial pooling")) +
  facet_wrap( ~ subject)
```
    
    v. in your own words, describe how the partial pooling model allows for a better fit for each subject 
It accommodates the individual subjects instead of using the data from all participants to make one graph. 



## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)  

```{r}
df_experiment <- df %>% 
  filter(trial.type == "experiment")
response_time <- lm(rt.obj ~ 1, data = df_experiment)
df_experiment$fitted_rt <- fitted(response_time)

```

1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modelled 
```{r}
pacman::p_load(car)

subject1 <- df_experiment %>% 
  filter(subject == "1")
subject2 <- df_experiment %>% 
  filter(subject == "2")
subject3 <- df_experiment %>% 
  filter(subject == "3")
subject4 <- df_experiment %>% 
  filter(subject == "4")

interceptmodel1 <- lm(rt.obj ~ 1, data = subject1)
interceptmodel2 <- lm(rt.obj ~ 1, data = subject2)
interceptmodel3 <- lm(rt.obj ~ 1, data = subject3)
interceptmodel4 <- lm(rt.obj ~ 1, data = subject4)

qqPlot(interceptmodel1)
qqPlot(interceptmodel2)
qqPlot(interceptmodel3)
qqPlot(interceptmodel4)

```

    i. comment on these   
They pretty bad... 

    ii. does a log-transformation of the response time data improve the Q-Q-plots?
```{r}
logsubject1 <- subject1 %>% 
  mutate(log_rt = log(rt.obj))
logsubject2 <- subject2 %>% 
  mutate(log_rt = log(rt.obj))
logsubject3 <- subject3 %>% 
  mutate(log_rt = log(rt.obj))
logsubject4 <- subject4 %>% 
  mutate(log_rt = log(rt.obj))

loginterceptmodel1 <- lm(log_rt ~ 1, data = logsubject1)
loginterceptmodel2 <- lm(log_rt ~ 1, data = logsubject2)
loginterceptmodel3 <- lm(log_rt ~ 1, data = logsubject3)
loginterceptmodel4 <- lm(log_rt ~ 1, data = logsubject4)

qqPlot(loginterceptmodel1)
qqPlot(loginterceptmodel2)
qqPlot(loginterceptmodel3)
qqPlot(loginterceptmodel4)

```


2) Now do a partial pooling model modelling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification) 
```{r}
#I should have added a new coloumn with the log-transformed rt.obj and used that for the rest of this exercise (since we just concluded that it makes the data more linear)

rt_partialpooling1 <- lmer(rt.obj ~ task + (1|subject), REML = FALSE, data = df_experiment)
rt_partialpooling2 <- lmer(rt.obj ~ task + (1|subject) + (1|trial), REML = FALSE, data = df_experiment)

```

    i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)  
Do some comparisons of the models. Maybe also make some more models. Maybe make an interceptmodel (rt.obj ~ 1). 

    ii. explain in your own words what your chosen models says about response times between the different tasks  


3) Now add _pas_ and its interaction with _task_ to the fixed effects 
```{r}
rt_partialpooling3 <- lmer(rt.obj ~ task*pas + (1|subject), REML = FALSE, data = df_experiment)
summary(rt_partialpooling3)
```

    i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits?
```{r}
rt_partialpooling4 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial), REML = FALSE, data = df_experiment)
rt_partialpooling5 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit), REML = FALSE, data = df_experiment)
rt_partialpooling6 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue), REML = FALSE, data = df_experiment)
rt_partialpooling7 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue) + (1|pas), REML = FALSE, data = df_experiment)
rt_partialpooling8 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue) + (1|pas) + (1|seed), REML = FALSE, data = df_experiment)
rt_partialpooling9 <- lmer(rt.obj ~ task*pas + (1|subject) + (1|trial) + (1|odd.digit) + (1|cue) + (1|pas) + (1|seed) + (1|even.digit), REML = FALSE, data = df_experiment) #Finally 
```


    ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)
```{r}
print(VarCorr(rt_partialpooling9), comp='Variance')

```
The fit is singular, because "even.digit" explains no variance. 

    iii. in your own words - how could you explain why your model would result in a singular fit? 
"even.digit" explains no variance, because I had already added "odd.digit", and they explain the samevariance. 

    
## Exercise 3

1) Initialise a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

```{r}
## you can start from this if you want to, but you can also make your own from scratch

#data.count <- data.frame(count = numeric(), 
                         #pas = numeric(), ## remember to make this into a factor afterwards
                         #task = numeric(), ## and this too
                         #subject = numeric()) ## and this too

data.count <- df %>%
  group_by(subject, task, pas) %>%
  dplyr::summarise("count" = n())


```        

2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled  
```{r}
pasmodel <- glmer(count ~ pas*task + (pas|subject), data = data.count, family = poisson, control = glmerControl(optimizer="bobyqa"))
summary(pasmodel)
```

    i. which family should be used?  
Poisson, because it's good for modelling counts. 

    ii. why is a slope for _pas_ not really being modelled?  
Doesn't really make sense to have a slope, when the values on the x-axis are not continuous, they are different categories. 

    iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)
Done... 

    iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction  
```{r}
pasmodel2 <- glmer(count ~ pas + task + (pas|subject), data = data.count, family = poisson)
summary(pasmodel2)

tibble(sum(residuals(pasmodel)^2), sum(residuals(pasmodel2)^2))
AIC(pasmodel, pasmodel2)

```


    v. indicate which of the two models, you would choose and why 
I would choose the one including the interaction. 

    vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_ 
Not sure... 

    vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing 
```{r}
pas_foursubjects <- data.count %>% 
  filter(subject == "1"|subject == "2"|subject == "3"|subject == "4")
pasmodel_foursubjects <- glmer(count ~ pas + task + (pas|subject), data = pas_foursubjects, family = poisson)

pas_foursubjects %>% 
  ggplot() +
  geom_point(aes(x = pas, y = fitted(pasmodel_foursubjects), color = "Estimated")) +
  geom_point(aes(x = pas, y = count, color = "Observed")) +
  facet_wrap( ~ subject)

```


3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_  
```{r}
correctmodel <- glmer(correct ~ task + (1|subject), data = df, family = "binomial")
summary(correctmodel)
```


    i. does _task_ explain performance?  
Yes. All p-values < 0.05. They have most correct in singles, then pairs and then quadruplets. Makes much sense. 

    ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
```{r}
correctmodel2 <- glmer(correct ~ task + pas + (1|subject), data = df, family = "binomial")
summary(correctmodel2)
```
Now the effects of the different tasks are not significant, but the effect of pas is, and the estimate is way higher, so it seems that pas is a way better predictor for correct answers. That makes pretty good sense, since pas stands for "perceptual awareness scale", so the participants rate how aware they were. 

    iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
```{r}
correctmodel3 <- glmer(correct ~ pas + (1|subject), data = df, family = "binomial")
summary(correctmodel3)

```
What a good model 

    iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects  
```{r}
correctmodel4 <- glmer(correct ~ task*pas + (1|subject), data = df, family = "binomial")
summary(correctmodel4)
```


    v. describe in your words which model is the best in explaining the variance in accuracy 
The best model we have made is the correctmodel3, which predicts correct by pas and has random intercepts per subject. It has the lowest AIC. 


